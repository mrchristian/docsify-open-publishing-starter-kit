<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <title>4. Reproducible Research and Data Analysis</title>
        <link type="text/css" rel="stylesheet" href="document.css" />
        <link rel="stylesheet" type="text/css" href="springer.css" />
    </head>
    <body>
        
        <div class="article-part article-title" id="_mb34bn7o2">4. Reproducible Research and Data Analysis</div><div class="article-part article-heading article-subtitle" data-hidden="true"></div><div class="article-part article-contributors article-authors" data-hidden="true"></div><div class="article-part article-richtext article-abstract" data-hidden="true"></div><div class="article-part article-tags article-keywords" data-hidden="true"></div><div class="article-part article-richtext article-body"><figure data-equation="" data-image="33" data-figure-category="none" data-caption="" id="F94030121" data-aligned="center" data-width="100" class="aligned-center image-width-100" data-image-src="/media/images/f5dde413-4c45-488f-8bc2-c2a57a02a812.png"><div><img src="f5dde413-4c45-488f-8bc2-c2a57a02a812.png"></div><figcaption></figcaption></figure><h1 id="H6230651">What is it?</h1><p>Reproducibility means that research data and code are made available so that others are able to reach the same results as are claimed in scientific outputs. Closely related is the concept of replicability, the act of repeating a scientific methodology to reach similar conclusions. These concepts are core elements of empirical research.</p><p>Improving reproducibility leads to increased rigour and quality of scientific outputs, and thus to greater trust in science. There has been a growing need and willingness to expose research workflows from initiation of a project and data collection right through to the interpretation and reporting of results. These developments have come with their own sets of challenges, including designing integrated research workflows that can be adopted by collaborators while maintaining high standards of integrity.</p><p>The concept of reproducibility is directly applied to the scientific method, the cornerstone of Science, and particularly to the following five steps:</p><ol><li><p>Formulating a hypothesis</p></li><li><p>Designing the study</p></li><li><p>Running the study and collecting the data</p></li><li><p>Analyzing the data</p></li><li><p>Reporting the study</p></li></ol><p>Each of these steps should be clearly reported by providing clear and open documentation, and thus making the study transparent and reproducible.</p><figure data-equation="" data-image="35" data-figure-category="none" data-caption="" id="F5873201" data-aligned="center" data-width="100" class="aligned-center image-width-100" data-image-src="/media/images/cd488cd3-09d0-48ea-a508-f6e5c12cf790.png"><div><img src="cd488cd3-09d0-48ea-a508-f6e5c12cf790.png"></div><figcaption></figcaption></figure><h1 id="rationale">Rationale</h1><p>Overarching factors can further contribute to the causes of non-reproducibility, but can also drive the implementation of specific measures to address these causes. The culture and environment in which research takes place is an important ‘top-down’ overarching factor. From a ‘bottom-up’ perspective, continuing education and training for researchers can raise awareness and disseminate good practice.</p><p>While understanding the full range of factors that contribute to reproducibility is important, it can also be hard to break down these factors into steps that can immediately be adopted into an existing research program and immediately improve its reproducibility. One of the first steps to take is to assess the current state of affairs, and to track improvement as steps are taken to increase reproducibility even more. Some of the common issues with research reproducibility are shown in the figure below.</p><figure data-equation="" data-image="37" data-figure-category="none" data-caption="" id="F27553871" data-aligned="center" data-width="100" class="aligned-center image-width-100" data-image-src="/media/images/651cc2af-b56a-42e6-b9e2-b072205db6ce.png"><div><img src="651cc2af-b56a-42e6-b9e2-b072205db6ce.png"></div><figcaption></figcaption></figure><p>Source: Symposium report, October 2015. Reproducibility and reliability of biomedical research: improving research practice <a href="https://acmedsci.ac.uk/viewFile/56314e40aac61.pdf">PDF</a>.</p><p><a href="https://doi.org/10/gc5sjs">Goodman, Fanelli, &amp; Ioannidis (2016)</a> note that in epidemiology, computational biology, economics, and clinical trials, reproducibility is often defined as:</p><p><em>"the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results."</em></p><p>This is distinct from replicability: <em>"which refers to the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected."</em> A simpler way of thinking about this might be that reproducibility is methods-oriented, whereas replicability is results-oriented.</p><p>Reproducibility can be assessed at several different levels: at the level of an individual project (e.g., a paper, an experiment, a method or a dataset), an individual researcher, a lab or research group, an institution, or even a research field. Slightly different kinds of criteria and points of assessment might apply to these different levels. For example, an institution upholds reproducibility practices if it institutes policies that reward researchers who conduct reproducible research. On the other hand, a research field might be considered to have a higher level of reproducibility if it develops community-maintained resources that promote and enable reproducible research practices, such as data repositories, or common data-sharing standards.</p><figure data-equation="" data-image="19" data-figure-category="none" data-caption="" id="F32455591" data-aligned="center" data-width="100" class="aligned-center image-width-100" data-image-src="/media/images/9209bd39-910a-46c7-a66a-21a7bbfcd5f7.png"><div><img src="9209bd39-910a-46c7-a66a-21a7bbfcd5f7.png"></div><figcaption></figcaption></figure><h1 id="learning-objectives">Learning objectives</h1><p>There are three major objectives that need to be addressed here:</p><ol><li><p>Understand the important impact of creating reproducible research.</p></li><li><p>Understand the overall setup of reproducible research (including workflow design, data management and dynamic reporting).</p></li><li><p>Be aware of the individual steps in the reproducibility process, as well as the corresponding resources that can be employed.</p></li></ol><figure data-equation="" data-image="21" data-figure-category="none" data-caption="" id="F15419331" data-aligned="center" data-width="100" class="aligned-center image-width-100" data-image-src="/media/images/27d237d8-8f65-4eb1-9c68-f087921c61b9.png"><div><img src="27d237d8-8f65-4eb1-9c68-f087921c61b9.png"></div><figcaption></figcaption></figure><h1 id="knowledge">Knowledge</h1><p>The following is an indicative list of take-away points on reproducibility:</p><ul><li><p>What is the ‘reproducibility crisis’, and meta-analyses of reproducibility.</p></li><li><p>Principles of reproducibility, and integrity and ethics in research.</p></li><li><p>What are the computing options and environments that allow collaborative and reproducible set up.</p></li><li><p>Factors that affect reproducibility of research.</p></li><li><p>Data analysis documentation and open research workflows.</p></li><li><p>Reproducible analysis environments (virtualization).</p></li><li><p>Addressing the "Researcher Degrees of Freedom" (<a href="https://doi.org/10/gc5sjn">Wicherts et al., 2016</a>).</p></li></ul><figure data-equation="" data-image="9" data-figure-category="none" data-caption="" id="F64172361" data-aligned="center" data-width="100" class="aligned-center image-width-100" data-image-src="/media/images/965a9b22-10b4-4501-a490-83443a94eafb.png"><div><img src="965a9b22-10b4-4501-a490-83443a94eafb.png"></div><figcaption></figcaption></figure><h1 id="skills">Skills</h1><p>There are several practical tips for reproducibility that one should have in mind when setting out the particular skills necessary to ensure this. Best practices in reproducibility borrow from Open Science practices more generally but their integration offers benefits to the individual researcher themselves, whether they choose to share their research or not. The reason that integrating reproducibility best practices benefits the individual researcher is that they improve the planning, organization, and documentation of research. Below we outline one example of implementing reproducibility into a research workflow with references to these practices in the handbook.</p><figure data-equation="" data-image="34" data-figure-category="none" data-caption="" id="F30251071" data-aligned="center" data-width="100" class="aligned-center image-width-100" data-image-src="/media/images/2f511b07-692b-4316-98d9-4918bfd7c688.png"><div><img src="2f511b07-692b-4316-98d9-4918bfd7c688.png"></div><figcaption></figcaption></figure><h2 id="1-plan-for-reproducibility-before-you-start"><strong>1. Plan for reproducibility before you start</strong></h2><h3 id="create-a-study-plan-or-protocol">Create a study plan or protocol.</h3><p>Begin documentation at study inception by writing a study plan or protocol that includes your proposed study design and methods. Use a reporting guideline from the <a href="http://www.equator-network.org/">Equator Network</a> if applicable. Track changes to your study plan or protocol using version control (reference to Version Control). Calculate the power or sample size needed and report this calculation in your protocol as underpowered studies are prone to irreproducibility.</p><h3 id="choose-reproducible-tools-and-materials">Choose reproducible tools and materials</h3><p>Select antibodies that work using an antibody search engine like <a href="https://www.citeab.com/">CiteAb</a>. Avoid irreproducibility through misidentified cell lines by choosing ones that are authenticated by the <a href="http://iclac.org/">International Cell Line Authentication Committee</a>. Whenever possible, choose software and hardware tools where you retain ownership of your research and can migrate your research out of the platform for reuse (see Open Research Software and Open Source).</p><h3 id="set-up-a-reproducible-project">Set-up a reproducible project</h3><p>Centralize and organize your project management using an online platform, a central repository, or folder for all research files. You could use GitHub as a place to store project files together or manage everything using a electronic lab notebook such as <a href="https://benchling.com/">Benchling</a>, <a href="https://www.labguru.com/">Labguru</a>,or <a href="https://scinote.net/">SciNote</a>. Within your centralized project, follow best practices by separating your data from your code into different folders. Make your raw data read-only and keep separate from processed data (reference to Data Management).</p><p>When saving and backing up your research files, choose formats and informative file names that allow for reuse. File names should be both machine and human readable (reference to Data Management). In your analysis and software code, use relative paths. Avoid proprietary file formats and use open file formats (see 6 Open Licensing and File Formats).</p><figure data-equation="" data-image="36" data-figure-category="none" data-caption="" id="F27277281" data-aligned="center" data-width="100" class="aligned-center image-width-100" data-image-src="/media/images/04fbcc59-de6d-400c-ae98-250e796e3621.png"><div><img src="04fbcc59-de6d-400c-ae98-250e796e3621.png"></div><figcaption></figcaption></figure><h2 id="2-keep-track-of-things"><strong>2. Keep track of things</strong></h2><h3 id="registration">Registration</h3><p>Preregister important study design and analysis information to increase transparency and counter publication bias of negative results. Free tools to help you make your first registration include <a href="https://aspredicted.org/">AsPredicted</a>, <a href="https://osf.io/">Open Science Framework</a>, and <a href="https://cos.io/rr/">Registered Reports</a>. Clinical trials should use <a href="https://clinicaltrials.gov/">Clinicaltrials.gov</a>.</p><h3 id="version-control">Version control</h3><p>Track changes to your files, especially your analysis code, using version control (see Open Research Software and Open Source).</p><h3 id="documentation">Documentation</h3><p>Document everything done by hand in a README file. Create a data dictionary (also known as a codebook) to describe important information about your data. For an easy introduction, use: <a href="http://kbroman.org/dataorg/pages/dictionary.html">Karl Broman’s Data Organization module</a> and refer to Data Management.</p><h3 id="literate-programming">Literate programming</h3><p>Consider using <a href="http://jupyter.org/">Jupyter Notebooks</a>, <a href="https://yihui.name/knitr/">KnitR</a>, <a href="https://support.rstudio.com/hc/en-us/articles/200552056-Using-Sweave-and-knitr">Sweave</a>, or other approaches to literate programming to integrate your code with your narrative and documentation.</p><figure data-equation="" data-image="10" data-figure-category="none" data-caption="" id="F41172011" data-aligned="center" data-width="100" class="aligned-center image-width-100" data-image-src="/media/images/bc14044b-eec3-4ed3-b0e7-e2156edefa58.png"><div><img src="bc14044b-eec3-4ed3-b0e7-e2156edefa58.png"></div><figcaption></figcaption></figure><h2 id="3-share-and-license-your-research"><strong>3. Share and license your research</strong></h2><h3 id="data">Data</h3><p>Avoid supplementary files, decide on an acceptable permissive license, and share your data using a repository. Follow best practices as outlined in the Open Research Data and Materials chapter.</p><h3 id="materials">Materials</h3><p>Share your materials so they can be reused. Deposit reagents with repositories like <a href="https://www.addgene.org/">Addgene</a>, <a href="https://bdsc.indiana.edu/">The Bloomington Drosophila Stock Center</a>, and <a href="https://www.atcc.org/">ATCC</a> to make them easily accessible to other researchers. For more information, see the Open Materials subsection of <a href="https://github.com/Open-Science-Training-Handbook/Open-Science-Training-Handbook_EN/blob/master/02OpenScienceBasics/02OpenResearchDataAndMaterials.md">Open Research Data and Materials</a>.</p><h3 id="software-notebooks-and-containers">Software, notebooks, and containers</h3><p>License your code to inform about how it may be (re)used. Share notebooks with services such as <a href="http://mybinder.org/">mybinder</a> that allow for public viewing and execution of the entire notebook on shared resources. Share containers or notebooks with services such as <a href="https://arxiv.org/abs/1710.03675">Rocker</a> or <a href="https://codeocean.com/">Code Ocean</a>. Follow best practices outlined in Open Research Software and Open Source.</p><figure data-equation="" data-image="38" data-figure-category="none" data-caption="" id="F98076721" data-aligned="center" data-width="100" class="aligned-center image-width-100" data-image-src="/media/images/9dfcfdf3-33f1-475f-81b9-0b157046b269.png"><div><img src="9dfcfdf3-33f1-475f-81b9-0b157046b269.png"></div><figcaption></figcaption></figure><h2 id="4-report-your-research-transparently"><strong>4. Report your research transparently</strong></h2><p>Report and publish your methods and interventions explicitly and transparently and fully to allow for replication. Guidelines from the <a href="http://www.equator-network.org/">Equator Network</a>, tools like <a href="https://www.protocols.io/">Protocols.io</a>, or processes like <a href="https://cos.io/rr/">Registered Reports</a> can help you report reproducibly. Remember to post your results to your public registration platform (such as <a href="https://www.socialscienceregistry.org/">ClinicalTrials.gov</a> or the <a href="https://www.socialscienceregistry.org/">SocialScienceRegistry</a>) within a year of finishing your study no matter the nature or direction of your results.</p><figure data-equation="" data-image="12" data-figure-category="none" data-caption="" id="F37038591" data-aligned="center" data-width="100" class="aligned-center image-width-100" data-image-src="/media/images/2b202773-ec41-4cde-8558-44800112ec6d.png"><div><img src="2b202773-ec41-4cde-8558-44800112ec6d.png"></div><figcaption></figcaption></figure><h1 id="questions-obstacles-and-common-misconceptions">Questions, obstacles, and common misconceptions</h1><p>Q: "Everything is in the paper; anyone can reproduce this from there!"</p><p>A: This is one of the most common misconceptions. Even having an extremely detailed description of the methods and workflows employed to reach the final result will not be sufficient in most cases to reproduce it. This can be due to several aspects, including different computational environments, differences in the software versions, implicit biases that were not clearly stated, etc.</p><p>Q: "I don’t have the time to learn and establish a reproducible workflow."</p><p>A: In addition to a significant number of freely available online services that can be combined and facilitate the setting up of an entire workflow, spending the time and effort to put this together will increase both the scientific validity of the final results as well as minimize the time of re-running or extending it in further studies.</p><p>Q: "Terminologies describing reproducibility are challenging."</p><p>A: See Barba (2018) for a discussion on terminology describing reproducibility and replicability.</p><figure data-equation="" data-image="18" data-figure-category="none" data-caption="" id="F67366301" data-aligned="center" data-width="100" class="aligned-center image-width-100" data-image-src="/media/images/78242a58-956c-4a6f-8d50-a93e47eb21b4.png"><div><img src="78242a58-956c-4a6f-8d50-a93e47eb21b4.png"></div><figcaption></figcaption></figure><h1 id="learning-outcomes">Learning outcomes</h1><ol><li><p>Understand the necessity of reproducible research and its reasoning.</p></li><li><p>Be able to establish a reproducible workflow within the context of an example task.</p></li><li><p>Know tools that can support reproducible research.</p></li></ol><figure data-equation="" data-image="17" data-figure-category="none" data-caption="" id="F56536271" data-aligned="center" data-width="100" class="aligned-center image-width-100" data-image-src="/media/images/28184297-5028-4ac9-bc5f-7a459edf50cf.png"><div><img src="28184297-5028-4ac9-bc5f-7a459edf50cf.png"></div><figcaption></figcaption></figure><h1 id="further-reading">Further reading</h1><ul><li><p>Button et al. (2013). Power failure: why small sample size undermines the reliability of neuroscience. <a href="https://doi.org/10.1038/nrn3475">doi.org/10.1038/nrn3475</a></p></li><li><p>Karl Broman (n.y.). Data Organization. Choose good names for things. <a href="http://kbroman.org/dataorg/pages/names.html">kbroman.org</a></p></li></ul></div><section class="fnlist" role="doc-footnotes"></section>
    </body>
</html>